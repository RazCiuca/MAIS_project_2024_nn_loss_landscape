
- if the edge of stability explains negative eigenvalues, then if there's no edge of stability, say we don't use
    weight decay, then there shouldn't be negative eigenvalues, correct?

- suppose you model the landscape as a baseline power law quadratic

3 pieces of evidence:
- negative eigenvalues persist despite their 1D instability
- at equilibrium with a given learning rate, they are at a maximum of the neg-eigenvector line searches
- the positive spetrum doesn't change much when we optimise high-eigen directions, but the negative spectrum does decrease uniformly

- other clue: this is not an output softmax temperature thing, that doesn't seem to matter

===============================================
Next major clue:
===============================================
- Edge of Stability only happens with weight decay!!!!??
- without weight decay top eigenvalues are much smaller, by a factor of 7 or 8, it hovers at 1 until the end
- though we still have negative eigenvalues even without weight decay, and they behave as expected, decreasing
  in magnitude as we drop the learning rate

- weight decay massively helps the edge of stability to manifest accurately.
- if we start with a given lr, then the eigenvalue will jump right away to that value, but it'll be stuck there,
    it won't go up much at successive lr decreases. Weight decay is needed for that.

- much more complicated. If we start with a given learning rate, it does happen, but the function doesn't sharpen
  after that if we decrease the learning rate, it only does so if we add weight-decay

- weird: the max eigenvalue at averaged networks is quite a bit smaller than the value of a random selection at equilibrium

- possible reason: the second derivative down the valley is absurdly tiny, and we need weight decay to help us down?

=================================================
negative eigenvalues explanation
=================================================

Here's the novel contribution:
- We show that the equilibrium distribution of SGD with a narrow valley stabilises at a local minimum of the negative
    eigenvalues
- People think that the negative eigenvalues of the Hessian are caused by saddle-like structures, but they're wrong.
    they are caused by narrowing valleys, not saddles. That's why they don't get optimised. And that's why the loss
    decreases when noise decreases
- The narrowing valley directions are the ones that cause overfitting.


- are we falling into the same sharp valley in different draws of batches?
    -> i.e. if we're at equilibrium at some learning rate, and we pick various different
       points to drop the learning rate, do we end up down the same valley?
    ->

- how much support does each eigendirection have over our examples?
    -> are the high eigenvalues due to all datapoints having medium contributions?
       or are a few examples really driving those values?
       what about the negative ones?

- possible change: