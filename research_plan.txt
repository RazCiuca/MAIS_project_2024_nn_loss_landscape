
MAIS project:

to investigate:
- how quadratic is the landscape in the top eigendirections? do line search around each eigenvalue.
- how does this function in the eigendirections change with finite batch size? what's the variation like
- how does the spectrum change as training progresses? are the eigendirections mostly consistent? is there a difference between low eigenvalues and high eigenvalues here? we need to multiply the two matrices together, and see the maximum of each row
- how do the parameters evolve in each eigendirection as training proceeds? at which iteration does the corresponding eigenvalue settle into equilibrium?
- which eigenvalues correspond to the overfitting regime? if we exactly fit low eigenvalues, but add some noise to the high eigenvalue ones, do we get better generalisation? or lower generalisation? what about uniformly adding noise to all eigenvalues? Use the validation set to set the noise level in each direction of eigenvalues using sgd, and report what the optimal noise level was.
- in the bayesian interpretation, you'd want to choose an algorithm with eigenvalue-dependent learning rate such that the expected visitation fraction is the bayesian posterior, of course.
- INSIGHT: for constant lr, there is exaclty one eigenvalue for which that learning rate corresponds to a random walk on the posterior. effectively, low learning rates do two things: they fit the high-eigenvalue directions more correctly, and they overfit the low-eigenvalue directions.
- Prediction for large networks: computing only a few really high eigenvalues, and being bayesian with them, will lead to better performance.
- also: compare models trained with different learning rates, with exactly the same initial configs, and the same sequence of batches, all annealed to the same end learning rate
- mysteries: can you really get to 0 loss with low learning rate (that would be a model prediction, not 0 error, but 0 loss, I don't think so)? why are warmups useful? The model prediction for why high lr is needed is to fit the low eigenvalue dimentions.
- fit the eigendirection functions to some simple functional form in order to try to get a general picture
- how quadratic are we really at the end? choose random directions to explore, and see how the quadratic-independence predictions hold up.


Title: The Quadratic Approximation to NN Loss Landscapes: explorations and implications.


- Step 1: network training
    - training 100k resnet on cifar10 to a reasonable performance, with lr decreases, using sgd + momentum
    - save almost all intermediate parameters
    - save all batch indices used over the course of training
    - train 2 other networks, without lr decreases, starting at lower lr

    relevant files: cifar10_test.py

- Step 2: hessian computing
    - choose a few checkpoints, and compute the hessian at these points, with the whole dataset
    - maybe 4 checkpoints? should take 2 days of computing, save all these parameters
    - find the eigenvalues and eigenvectors of all these hessians

    relevant files: HessianEigen_utils.py

- Step 3: finding functional forms at minimum
    - at the found minimum, do line search in a selection of eigendirection and save the resulting functions
    - also compute these functions with small batch size, find the distribution of minimums, and the distribution of eigenvalues, is there a correlation here?

- Step 4: Analysis
    - do we get to a lower loss if we only train with smaller lr? the hypothesis is that time spent at high lr is to optimise low-eigenvalue directions.

    - how does the spectrum of eigenvalues evolve through training?

    - are the eigenvectors mostly constant? despite the spectrum changing?

    - how far from minimum in each eigendirection does the network start?

    - at which iteration do the corresponding eigenvalues (at the minimum) settle into equilibrium and start to oscillate?

    - how good is the eigen-independent approximation? do line search in random directions and see how the predictions hold up

    - what is the form of the stochasticity in each eigendirection? does the variance in minimum location depend on eigenvalue?

    - which eigenvalues contribute more to overfitting?
        -> train again for a bit with bayesian-inspired learning rates, and see if the performance goes up
        -> choose a threshold of eigenvalues to set exactly to their minima, and see how performance improves as function of the threshold

    - do we have any fractal pattern to the full loss?
        -> do a few really, really dense line searches near the minimum

    - are the low eigenvalues caused by lots of examples having slight dependence in that direction, or a single, high-curvature example?
        -> compute all single example gradients, decompose them into the eigenbasis, and see if high-eigenvalue directions just hold more of the dot product
        -> plot the distribution of absolute value of cosine angles between normalized gradients and eigenvalues, with
           respect to each eigenvalue, and see how the distribution shifts

- Step 5: Theory and Predictions of the Model which are validated experimentally
    - given the evidence we've seen, we can derive an optimal learning rate schedule for our network, from
      theoretical considerations from using sgd + momentum.
    - Do we get better generalisation if we make our oscillations in scale with the bayesian posterior?
      to do this, change the learning rates per dimension to scale with the eigenvalue, and observe generalisation.
      then observe the generalisation as we vary learning rate
    - do the analysis for nesterov momentu, and for an arbitrary update sequence in the past, then optimize
      the sequence parameters to do well for the type of power law spectrum we have.
    - prediction: at the very end, decrease the learning rate to almost nothing and do a full epoch
      with batch_size = 1, the prediction is that high noise variance leads to better matching the bayesian
      posterior sampling
    - prediction: doing HMC at the end will also increase the accuracy of the model

steps:
- computing hessians with finite batch size
    -> compute the top few hessians of whole dataset, and make sure that things make sense
- computing line interpolations
- writing scripts for Analyses
- writing paper
- making poster (and printing)

3 weeks in total

today: start the hessian computation, and see how long it takes, save the hessians to disk
maybe 4 points of interest: non-optimized, then only a few iterations afterwards, then middle of training, then end

iters = 0, 2000, 20000, 48800
at 14 hours per hessian, this is roughly 2 days of computing



